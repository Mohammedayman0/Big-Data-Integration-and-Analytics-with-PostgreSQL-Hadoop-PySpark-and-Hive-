# Big-Data-Integration-and-Analytics-with-PostgreSQL-Hadoop-PySpark-and-Hive-
Here's a brief overview of what I accomplished:

ðŸ”¹ Project Goals:

Extract data from a PostgreSQL database.

Transform the data using PySpark.

Load the transformed data into Hadoop as Parquet files.

Build and implement a star schema data model.

Save the data into Hive tables for easy querying and analysis.

ðŸ”¹ Technologies Used:

PostgreSQL: As the source database for extracting data.

Hadoop: For distributed storage of data.

PySpark: For data processing and transformation.

Hive: For managing and querying large datasets stored in Hadoop.

ðŸ”¹ Key Steps:

1-Data Extraction: Utilized PySpark to connect to PostgreSQL and read data into DataFrames after read data from csv files into database.

2-Data Transformation: Processed and transformed data into a star schema format using PySpark.

3-Data Loading: Saved the transformed data into Hadoop as Parquet files.

4-Star Schema Implementation: Built a star schema with fact and dimension tables.

5-Data Storage in Hive: Saved the star schema tables into Hive for efficient querying.
